{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9b24a0",
   "metadata": {},
   "source": [
    "# LangChain Simplified Tutorial\n",
    "## Real Implementation with OpenAI - Student Educational Support System\n",
    "\n",
    "### Learning Objectives\n",
    "- Master core LangChain components: Models, Prompts, Chains, Memory, Retrieval, Tools, Agents\n",
    "- Build a complete educational support chatbot from start to finish\n",
    "- Understand how components work together in a real application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76997d",
   "metadata": {},
   "source": [
    "### All Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5072ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Memory & Chains\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain, LLMChain, RetrievalQA\n",
    "\n",
    "# Retrieval & Vector Stores\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Tools & Agents\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25402c",
   "metadata": {},
   "source": [
    "### Environment Setup and API Key Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19db253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key found and loaded\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"‚ùå OPENAI_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found and loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8afec0",
   "metadata": {},
   "source": [
    "## 1. Models - Basic Setup\n",
    "**Models are the AI brains (like GPT-4) that understand your questions and generate intelligent responses.**\n",
    "\n",
    "### Understanding Message Types in LangChain\n",
    "\n",
    "This is how LangChain keeps track of who said what in a conversation ‚Äî it uses different message types for different roles:\n",
    "\n",
    "- **HumanMessage** ‚Üí input from the user (like you typing a query).\n",
    "- **AIMessage** ‚Üí responses generated by the AI model.\n",
    "- **SystemMessage** ‚Üí system-level instructions (e.g., context, rules, or prompts that guide the AI's behavior).\n",
    "\n",
    "So in your code:\n",
    "```python\n",
    "response = llm.invoke([HumanMessage(content=test_input)])\n",
    "```\n",
    "\n",
    "You're passing the model a list of messages, where the first message is from the human (you). The model then replies with an AIMessage.\n",
    "\n",
    "üëâ If you just passed a raw string, it wouldn't know who the message was from, but with HumanMessage, it knows it's your input in a dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d98b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ INPUT TO GPT:\n",
      "Query: What are the top 3 benefits of online learning platforms?\n",
      "\n",
      "üîµ CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT:\n",
      "Online learning platforms offer numerous advantages, but here are the top three benefits:\n",
      "\n",
      "1. **Flexibility and Convenience**: Online learning allows students to access courses and materials at their own pace and on their own schedule. This flexibility is particularly beneficial for those balancing work, family, or other commitments, as it enables learners to study when it is most convenient for them.\n",
      "\n",
      "2. **Wide Range of Courses and Resources**: Online platforms provide access to a vast array of subjects and courses that may not be available locally. Learners can explore diverse topics, from academic subjects to professional development and personal interests, often taught by experts from around the world.\n",
      "\n",
      "3. **Cost-Effectiveness**: Many online learning platforms offer courses at a lower cost compared to traditional education. Additionally, learners can save on commuting, housing, and other expenses associated with in-person education. Many platforms also provide free resources or financial aid options, making education more accessible to a broader audience. \n",
      "\n",
      "These benefits contribute to the growing popularity of online learning as a viable alternative to traditional educational methods.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_input = \"What are the top 3 benefits of online learning platforms?\"\n",
    "\n",
    "print(\"üîµ INPUT TO GPT:\")\n",
    "print(f\"Query: {test_input}\")\n",
    "\n",
    "print(\"\\nüîµ CALLING GPT-4O-MINI...\")\n",
    "response = llm.invoke([HumanMessage(content=test_input)])\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cebc713",
   "metadata": {},
   "source": [
    "## 2. Prompts and Templates\n",
    "**Prompts are structured instructions with placeholders that tell the model exactly what to do with your data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8b8b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ INPUT TO GPT:\n",
      "Template: Educational Support Analysis\n",
      "Student Query: I can't access my course materials and have an exam tomorrow!\n",
      "Formatted Messages: ['You are an educational support specialist. Be encouraging and helpful.', \"Analyze this student query: I can't access my course materials and have an exam tomorrow!\", 'Provide urgency level (1-5) and suggested response category.']\n",
      "\n",
      "üîµ CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT:\n",
      "Urgency Level: 5 (Critical)\n",
      "\n",
      "Suggested Response Category: Immediate Assistance\n",
      "\n",
      "---\n",
      "\n",
      "**Response:**\n",
      "\n",
      "Hi there! I understand how stressful it can be to not have access to your course materials, especially with an exam tomorrow. Let's work together to resolve this as quickly as possible. \n",
      "\n",
      "1. **Check Your Login Credentials:** Make sure you‚Äôre using the correct username and password. Sometimes a simple typo can cause access issues.\n",
      "\n",
      "2. **Clear Your Browser Cache:** Sometimes, clearing your browser's cache can help with loading issues.\n",
      "\n",
      "3. **Try a Different Browser or Device:** If possible, try accessing your course materials from a different web browser or device.\n",
      "\n",
      "4. **Contact Technical Support:** If you‚Äôre still having trouble, please reach out to your institution's technical support team immediately. They can provide you with the quickest assistance.\n",
      "\n",
      "5. **Reach Out to Your Instructor:** If you can‚Äôt resolve the issue in time, consider emailing your instructor to explain the situation. They may be able to provide you with the materials directly or offer an alternative solution.\n",
      "\n",
      "Remember, you‚Äôre not alone in this, and it‚Äôs okay to ask for help. Good luck with your exam!\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Basic Educational Support Template\n",
    "student_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an educational support specialist. Be encouraging and helpful.\"),\n",
    "    (\"human\", \"Analyze this student query: {student_query}\"),\n",
    "    (\"human\", \"Provide urgency level (1-5) and suggested response category.\")\n",
    "])\n",
    "\n",
    "# Format and use the prompt\n",
    "student_query = \"I can't access my course materials and have an exam tomorrow!\"\n",
    "formatted_prompt = student_prompt.format_messages(student_query=student_query)\n",
    "\n",
    "print(\"üîµ INPUT TO GPT:\")\n",
    "print(\"Template: Educational Support Analysis\")\n",
    "print(f\"Student Query: {student_query}\")\n",
    "print(f\"Formatted Messages: {[msg.content for msg in formatted_prompt]}\")\n",
    "\n",
    "print(\"\\nüîµ CALLING GPT-4O-MINI...\")\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50ef0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ INPUT TO GPT:\n",
      "Template: Multi-role Support Response\n",
      "Variables: {'role': 'Senior Academic Advisor', 'platform': 'EduTech Learning Platform', 'tone': 'supportive and encouraging', 'issue': 'Struggling with advanced calculus concepts', 'context': 'Student is in their second year, enrolled in premium tutoring'}\n",
      "\n",
      "üîµ CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT:\n",
      "Dear [Student's Name],\n",
      "\n",
      "Thank you for reaching out and sharing your concerns about struggling with advanced calculus concepts. First and foremost, I want you to know that it's completely normal to fin...\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Multi-role Conversation Template\n",
    "support_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role} at {platform}. Use {tone} tone.\"),\n",
    "    (\"human\", \"Student issue: {issue}\"),\n",
    "    (\"human\", \"Previous context: {context}\"),\n",
    "    (\"human\", \"Generate a response that addresses the issue and follows up appropriately.\")\n",
    "])\n",
    "\n",
    "# Example usage\n",
    "response_inputs = {\n",
    "    \"role\": \"Senior Academic Advisor\",\n",
    "    \"platform\": \"EduTech Learning Platform\", \n",
    "    \"tone\": \"supportive and encouraging\",\n",
    "    \"issue\": \"Struggling with advanced calculus concepts\",\n",
    "    \"context\": \"Student is in their second year, enrolled in premium tutoring\"\n",
    "}\n",
    "\n",
    "formatted_chat = support_template.format_messages(**response_inputs)\n",
    "\n",
    "print(\"\\nüîµ INPUT TO GPT:\")\n",
    "print(\"Template: Multi-role Support Response\")\n",
    "print(f\"Variables: {response_inputs}\")\n",
    "\n",
    "print(\"\\nüîµ CALLING GPT-4O-MINI...\")\n",
    "response = llm.invoke(formatted_chat)\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT:\")\n",
    "print(response.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97c417",
   "metadata": {},
   "source": [
    "## 3. Chains - Connecting Components\n",
    "**Chains link multiple steps together (prompt ‚Üí model ‚Üí parser) to create reusable AI workflows.**\n",
    "\n",
    "### Understanding the Pipe Operator (|) in LangChain\n",
    "\n",
    "#### What does | mean here?\n",
    "\n",
    "Yes ‚Äî in LangChain's new \"LangChain Expression Language\" (LCEL), the `|` is operator overloading for pipe composition.\n",
    "\n",
    "It works a lot like a Unix shell pipeline (`cmd1 | cmd2 | cmd3`), but instead of passing raw text, each component in LangChain has a well-defined input/output schema. The pipe (`|`) just wires them together.\n",
    "\n",
    "So this:\n",
    "```python\n",
    "solution_template | llm | StrOutputParser()\n",
    "```\n",
    "\n",
    "means:\n",
    "1. Take the PromptTemplate (solution_template)\n",
    "2. Format it with the given inputs ‚Üí produces a prompt string (or messages)\n",
    "3. Send it to llm (the LLM model) ‚Üí produces an LLM response\n",
    "4. Pass the response into StrOutputParser() ‚Üí returns a clean string\n",
    "\n",
    "Each `|` passes the output of the left component as input to the right component, creating a seamless data flow pipeline.\n",
    "\n",
    "#### Chain vs Manual Steps Comparison\n",
    "\n",
    "**‚ùå Manual Approach (3 separate steps):**\n",
    "```python\n",
    "# Step 1: Format the prompt\n",
    "formatted_prompt = prompt.format_messages(metric=metric_input)\n",
    "\n",
    "# Step 2: Call the LLM\n",
    "raw_response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Step 3: Parse the output\n",
    "result = parser.invoke(raw_response)\n",
    "```\n",
    "\n",
    "**‚úÖ Chain Approach (1 simple step):**\n",
    "```python\n",
    "# All steps combined into one seamless pipeline\n",
    "analysis_chain = prompt | llm | parser\n",
    "result = analysis_chain.invoke({\"metric\": metric_input})\n",
    "```\n",
    "\n",
    "**Benefits of Chains:**\n",
    "- **Less Code**: 1 line instead of 3 separate operations\n",
    "- **No Intermediate Variables**: No need to manage `formatted_prompt` or `raw_response`\n",
    "- **Automatic Data Flow**: Each component's output automatically becomes the next component's input\n",
    "- **Reusable Pipeline**: Create once, use multiple times with different inputs\n",
    "- **Error Handling**: Built-in error propagation through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cc9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ INPUT TO GPT:\n",
      "Chain: Educational Data Analysis\n",
      "Metric: Course completion rate dropped by 20%\n",
      "\n",
      "üîµ CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT:\n",
      "A 20% drop in course completion rate is a significant change that warrants a thorough analysis to understand the underlying causes and implications. Here are several factors to consider when analyzing this learning metric:\n",
      "\n",
      "### 1. **Contextual Factors**\n",
      "   - **Time Frame**: When did the drop occur? Was it during a specific term, semester, or after a particular event (e.g., transition to online learning)?\n",
      "   - **Course Characteristics**: Are there specific courses that experienced a more significant drop? Are these courses known for being particularly challenging or requiring more resources?\n",
      "\n",
      "### 2. **Potential Causes**\n",
      "   - **Student Engagement**: A decrease in student engagement could lead to lower completion rates. This might be due to factors such as lack of motivation, increased distractions, or changes in personal circumstances (e.g., work, family).\n",
      "   - **Course Design**: Changes in course structure, content delivery, or assessment methods could impact completion rates. For instance, if the course became more difficult or less interactive, students might be less likely to finish.\n",
      "   - **Support Services**: A reduction in academic support services (e.g., tutoring, counseling) could affect students' ability to complete courses.\n",
      "   - **External Factors**: Consider external factors such as economic conditions, health crises (like the COVID-19 pandemic), or changes in institutional policies that might affect students' ability to complete courses.\n",
      "\n",
      "### 3. **Demographic Analysis**\n",
      "   - **Student Demographics**: Analyze whether the drop in completion rates affects specific demographic groups (e.g., first-generation students, part-time students, or students from underrepresented backgrounds).\n",
      "   - **Enrollment Trends**: Look at enrollment numbers to see if there has been a shift in the type of students enrolling in courses, which might correlate with the completion rate drop.\n",
      "\n",
      "### 4. **Comparative Analysis**\n",
      "   - **Historical Data**: Compare the current completion rate with historical data to identify trends over time. Is this drop an anomaly, or is it part of a longer-term trend?\n",
      "   - **Benchmarking**: Compare the completion rates with similar institutions or programs to see if this is a widespread issue or specific to your institution.\n",
      "\n",
      "### 5. **Implications**\n",
      "   - **Academic Performance**: A lower completion rate can indicate potential issues with student learning outcomes and overall academic performance.\n",
      "   - **Institutional Reputation**: A significant drop in completion rates can affect the institution's reputation and attractiveness to prospective students.\n",
      "   - **Funding and Resources**: Many funding models are tied to completion rates, so a drop could impact financial resources available for programs and services.\n",
      "\n",
      "### 6. **Actionable Steps**\n",
      "   - **Surveys and Feedback**: Conduct surveys or focus groups with students to gather qualitative data on their experiences and challenges.\n",
      "   - **Intervention Strategies**: Develop targeted interventions, such as enhanced academic support, mentorship programs, or course redesigns to improve engagement and completion rates.\n",
      "   - **Monitoring and Evaluation**: Implement a system for ongoing monitoring of completion rates and related metrics to assess the effectiveness of any interventions.\n",
      "\n",
      "### Conclusion\n",
      "A 20% drop in course completion rates is a critical issue that requires immediate attention. By analyzing the various factors contributing to this decline and implementing targeted strategies, institutions can work towards improving student outcomes and enhancing the overall learning experience.\n"
     ]
    }
   ],
   "source": [
    "# Create educational data analysis chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an educational data analyst.\"),\n",
    "    (\"human\", \"Analyze this learning metric: {metric}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Create chain using pipe operator\n",
    "analysis_chain = prompt | llm | parser\n",
    "\n",
    "# Use the chain\n",
    "metric_input = \"Course completion rate dropped by 20%\"\n",
    "\n",
    "print(\"üîµ INPUT TO GPT:\")\n",
    "print(\"Chain: Educational Data Analysis\")\n",
    "print(f\"Metric: {metric_input}\")\n",
    "\n",
    "print(\"\\nüîµ CALLING GPT-4O-MINI...\")\n",
    "result = analysis_chain.invoke({\"metric\": metric_input})\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286eda2a",
   "metadata": {},
   "source": [
    "## 4. Memory - Conversation Context\n",
    "**Memory stores conversation history so the AI remembers what was discussed earlier in the chat.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f79c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ INPUT TO GPT:\n",
      "Conversation with Memory - Turn 1\n",
      "Student: Hi, I'm Alice and I'm struggling with my CS101 course\n",
      "\n",
      "üîµ CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT:\n",
      "AI: Hello Alice! I'm here to help you out. CS101 can be quite challenging, especially if you're just starting out with computer science concepts. What specific topics are you struggling with? Is it programming, algorithms, data structures, or something else? Let‚Äôs tackle it together!\n",
      "\n",
      "üîµ INPUT TO GPT:\n",
      "Conversation with Memory - Turn 2\n",
      "Student: What's my name and what course am I taking?\n",
      "\n",
      "üîµ CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT:\n",
      "AI: Your name is Alice, and you're taking a CS101 course. How's it going so far? Are there any particular topics or assignments that are giving you trouble?\n",
      "\n",
      "üìù Memory Buffer:\n",
      "Human: Hi, I'm Alice and I'm struggling with my CS101 course\n",
      "AI: Hello Alice! I'm here to help you out. CS101 can be quite challenging, especially if you're just starting out with computer science concepts. What specific topics are you struggling with? Is it programming, algorithms, data structures, or something else? Let‚Äôs tackle it together!\n",
      "Human: What's my name and what course am I taking?\n",
      "AI: Your name is Alice, and you're taking a CS101 course. How's it going so far? Are there any particular topics or assignments that are giving you trouble?\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "input1 = \"Hi, I'm Alice and I'm struggling with my CS101 course\"\n",
    "\n",
    "print(\"üîµ INPUT TO GPT:\")\n",
    "print(\"Conversation with Memory - Turn 1\")\n",
    "print(f\"Student: {input1}\")\n",
    "\n",
    "print(\"\\nüîµ CALLING GPT-4O-MINI...\")\n",
    "response1 = conversation.invoke({\"input\": input1})[\"response\"]\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT:\")\n",
    "print(\"AI:\", response1)\n",
    "\n",
    "input2 = \"What's my name and what course am I taking?\"\n",
    "\n",
    "print(\"\\nüîµ INPUT TO GPT:\")\n",
    "print(\"Conversation with Memory - Turn 2\")\n",
    "print(f\"Student: {input2}\")\n",
    "\n",
    "print(\"\\nüîµ CALLING GPT-4O-MINI...\")\n",
    "response2 = conversation.invoke({\"input\": input2})[\"response\"]\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT:\")\n",
    "print(\"AI:\", response2)\n",
    "\n",
    "print(\"\\nüìù Memory Buffer:\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29680f",
   "metadata": {},
   "source": [
    "## 5. Retrieval - Knowledge Base Integration\n",
    "**Retrieval searches through documents/databases to find relevant information before answering questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4179b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Creating Knowledge Base...\n"
     ]
    }
   ],
   "source": [
    "# Sample educational policies for knowledge base\n",
    "educational_policies = [\n",
    "    \"Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds available until week 4.\",\n",
    "    \"Grade Appeal Process: Students must submit grade appeals within 2 weeks of grade posting. Appeals are reviewed by academic committee.\",\n",
    "    \"Academic Probation: Students with GPA below 2.0 for two consecutive semesters are placed on academic probation.\",\n",
    "    \"Extension Policy: Course extensions up to 30 days require instructor approval. Extensions over 30 days need academic advisor approval.\",\n",
    "    \"Technical Support: All students receive email support within 24 hours. Priority support available for premium students.\",\n",
    "    \"Graduation Requirements: Students must complete 120 credits with minimum 2.0 GPA and all core course requirements.\"\n",
    "]\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "print(\"üìö Creating Knowledge Base...\")\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=educational_policies,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1cfc6",
   "metadata": {},
   "source": [
    "### Understanding RetrievalQA.from_chain_type() Components\n",
    "\n",
    "```python\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "```\n",
    "\n",
    "**ü§î Why `from_chain_type()` instead of direct initialization?**\n",
    "\n",
    "`RetrievalQA` is a complex chain that needs to coordinate multiple steps:\n",
    "1. **Retrieve** relevant documents from vector store\n",
    "2. **Combine** retrieved text with user question  \n",
    "3. **Generate** answer using LLM\n",
    "\n",
    "`from_chain_type()` is a **factory method** that automatically builds the right chain configuration for you, instead of manually wiring these components together.\n",
    "\n",
    "**üóÇÔ∏è What is `chain_type=\"stuff\"`?**\n",
    "\n",
    "The \"stuff\" method determines **how retrieved documents are combined** before sending to the LLM:\n",
    "\n",
    "| Chain Type | How It Works | Best For | Example |\n",
    "|------------|--------------|----------|---------|\n",
    "| **\"stuff\"** | Concatenates ALL retrieved docs into one prompt | Short docs, simple questions | \"Here are 3 policy documents: [doc1][doc2][doc3]. Question: What's the drop policy?\" |\n",
    "| **\"map_reduce\"** | Summarizes each doc separately, then combines summaries | Long docs, complex analysis | Step 1: Summarize each doc ‚Üí Step 2: Combine summaries ‚Üí Answer |\n",
    "| **\"refine\"** | Builds answer iteratively, refining with each doc | Sequential reasoning needed | Answer using doc1 ‚Üí Refine answer with doc2 ‚Üí Final refine with doc3 |\n",
    "| **\"map_rerank\"** | Scores each doc's relevance, uses highest-scoring one | When only 1 best source needed | Score doc1: 0.9, doc2: 0.7, doc3: 0.8 ‚Üí Use doc1 only |\n",
    "\n",
    "**\"stuff\"** is the **most common and fastest** approach for typical Q&A scenarios.\n",
    "\n",
    "**üîç What is `vectorstore.as_retriever()`?**\n",
    "\n",
    "**Vector Store** vs **Retriever** serve different purposes:\n",
    "\n",
    "- **VectorStore** (`Chroma`) = Database that stores and searches embeddings\n",
    "  - Methods: `.similarity_search()`, `.add_texts()`, `.persist()`\n",
    "  - Direct database operations\n",
    "\n",
    "- **Retriever** = Standardized interface for getting relevant documents  \n",
    "  - Methods: `.get_relevant_documents()`, `.invoke()`\n",
    "  - Works with any retrieval system (vector, keyword, hybrid)\n",
    "\n",
    "**Why convert?**\n",
    "```python\n",
    "# ‚ùå RetrievalQA expects a \"retriever\" interface, not a \"vectorstore\" \n",
    "RetrievalQA.from_chain_type(retriever=vectorstore)  # TypeError!\n",
    "\n",
    "# ‚úÖ Convert vectorstore to retriever interface\n",
    "RetrievalQA.from_chain_type(retriever=vectorstore.as_retriever())  # Works!\n",
    "```\n",
    "\n",
    "**Optional retriever configuration:**\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",     # or \"mmr\" (max marginal relevance) \n",
    "    search_kwargs={\"k\": 3}        # return top 3 documents\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12b8f68",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ INPUT TO RETRIEVAL QA:\n",
      "Question: What is the course drop policy?\n",
      "Action: Search knowledge base + Generate answer\n",
      "\n",
      "üîµ SEARCHING KNOWLEDGE BASE AND CALLING GPT-4O-MINI...\n",
      "\n",
      "ü§ñ OUTPUT FROM GPT (with retrieved context):\n",
      "Answer: Students can drop courses within 2 weeks for a full refund. Partial refunds are available until week 4.\n",
      "Sources: ['Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds availa...', 'Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds availa...', 'Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds availa...', 'Extension Policy: Course extensions up to 30 days require instructor approval. Extensions over 30 da...']\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "question = \"What is the course drop policy?\"\n",
    "\n",
    "print(f\"\\nüîµ INPUT TO RETRIEVAL QA:\")\n",
    "print(f\"Question: {question}\")\n",
    "print(\"Action: Search knowledge base + Generate answer\")\n",
    "\n",
    "print(\"\\nüîµ SEARCHING KNOWLEDGE BASE AND CALLING GPT-4O-MINI...\")\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(\"\\nü§ñ OUTPUT FROM GPT (with retrieved context):\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(f\"Sources: {[doc.page_content[:100] + '...' for doc in result['source_documents']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb44163",
   "metadata": {},
   "source": [
    "## 6. PDF Processing with Chroma DB - Document Q&A\n",
    "**PDF Processing extracts text from PDFs and creates searchable vector databases for intelligent Q&A.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76e634",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_process_pdf(pdf_path):\n",
    "    \"\"\"Load PDF and process it for Chroma DB\"\"\"\n",
    "    \n",
    "    print(\"üìö Loading PDF document...\")\n",
    "    print(f\"File: {pdf_path}\")\n",
    "    \n",
    "    # Load PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(pages)} pages from PDF\")\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    print(f\"üìù Split into {len(docs)} text chunks\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings_pdf = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    \n",
    "    # Clear any existing chroma database (commented out to prevent errors on rerun)\n",
    "    db_path = \"./pdf_knowledge_base\"\n",
    "    # if os.path.exists(db_path):\n",
    "    #     shutil.rmtree(db_path)\n",
    "    #     print(\"üßπ Cleared existing database\")\n",
    "    \n",
    "    # Create Chroma vector store\n",
    "    print(\"üîç Creating vector embeddings...\")\n",
    "    vectorstore_pdf = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings_pdf,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    # Persist the database\n",
    "    vectorstore_pdf.persist()\n",
    "    print(f\"üíæ Vector database saved to: {db_path}\")\n",
    "    \n",
    "    return vectorstore_pdf\n",
    "\n",
    "def create_pdf_qa_system(vectorstore_pdf):\n",
    "    \"\"\"Create Q&A system for PDF documents\"\"\"\n",
    "    \n",
    "    # Create retrieval QA chain\n",
    "    qa_chain_pdf = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore_pdf.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}  # Return top 3 relevant chunks\n",
    "        ),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ PDF Q&A system created successfully\")\n",
    "    return qa_chain_pdf\n",
    "\n",
    "# Process the PDF (assuming educational_course_handbook.pdf exists)\n",
    "pdf_file_path = \"educational_course_handbook.pdf\"\n",
    "\n",
    "print(\"üöÄ Starting PDF Processing Pipeline...\")\n",
    "pdf_vectorstore = load_and_process_pdf(pdf_file_path)\n",
    "\n",
    "# Create Q&A system\n",
    "pdf_qa_system = create_pdf_qa_system(pdf_vectorstore)\n",
    "\n",
    "# Test the PDF Q&A system\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TESTING PDF Q&A SYSTEM\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Educational questions to test\n",
    "pdf_questions = [\n",
    "    \"What is the attendance policy?\",\n",
    "    \"How do I appeal a grade?\", \n",
    "    \"What academic support services are available?\"\n",
    "]\n",
    "\n",
    "for question in pdf_questions:\n",
    "    print(f\"\\nüîµ INPUT TO PDF Q&A SYSTEM:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Action: Search PDF content + Generate answer\")\n",
    "    \n",
    "    print(f\"\\nüîµ SEARCHING PDF DATABASE AND CALLING GPT-4O-MINI...\")\n",
    "    \n",
    "    result = pdf_qa_system.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nü§ñ OUTPUT FROM GPT (with PDF context):\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(f\"üìÑ Sources: Page {result['source_documents'][0].metadata.get('page', 'Unknown')} of PDF\")\n",
    "    print(f\"üìù Source Text Preview: {result['source_documents'][0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be94fc",
   "metadata": {},
   "source": [
    "## 7. Tools - Function Calling\n",
    "**Tools are custom functions the AI can call to perform specific actions like calculations, lookups, or API calls.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6627b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define educational tools\n",
    "@tool\n",
    "def get_student_info(student_id: str) -> str:\n",
    "    \"\"\"Get student information from learning management system\"\"\"\n",
    "    students = {\n",
    "        \"STU001\": {\n",
    "            \"name\": \"Alice Johnson\",\n",
    "            \"email\": \"alice.johnson@university.edu\",\n",
    "            \"major\": \"Computer Science\", \n",
    "            \"gpa\": 3.7,\n",
    "            \"current_courses\": [\"CS101\", \"MATH201\", \"PHYS101\"],\n",
    "            \"support_level\": \"Premium\"\n",
    "        }\n",
    "    }\n",
    "    student = students.get(student_id, {\"error\": \"Student not found\"})\n",
    "    return json.dumps(student, indent=2)\n",
    "\n",
    "@tool\n",
    "def calculate_grade(points_earned: int, total_points: int) -> str:\n",
    "    \"\"\"Calculate percentage grade and letter grade\"\"\"\n",
    "    if total_points == 0:\n",
    "        return \"Error: Total points cannot be zero\"\n",
    "    \n",
    "    percentage = (points_earned / total_points) * 100\n",
    "    \n",
    "    if percentage >= 90:\n",
    "        letter = \"A\"\n",
    "    elif percentage >= 80:\n",
    "        letter = \"B\"\n",
    "    elif percentage >= 70:\n",
    "        letter = \"C\"\n",
    "    elif percentage >= 60:\n",
    "        letter = \"D\"\n",
    "    else:\n",
    "        letter = \"F\"\n",
    "    \n",
    "    return f\"Grade: {percentage:.1f}% ({letter})\"\n",
    "\n",
    "# Test tools directly\n",
    "print(\"üîß Testing Tools:\")\n",
    "\n",
    "print(\"\\nüîµ INPUT TO TOOL:\")\n",
    "print(\"Tool: Grade Calculator\")\n",
    "print(\"Input: 85 points out of 100\")\n",
    "\n",
    "grade_result = calculate_grade.invoke({\"points_earned\": 85, \"total_points\": 100})\n",
    "print(f\"ü§ñ TOOL OUTPUT: {grade_result}\")\n",
    "\n",
    "print(\"\\nüîµ INPUT TO TOOL:\")\n",
    "print(\"Tool: Student Lookup\")\n",
    "print(\"Input: STU001\")\n",
    "\n",
    "student_result = get_student_info.invoke({\"student_id\": \"STU001\"})\n",
    "print(f\"ü§ñ TOOL OUTPUT:\\n{student_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d8a76",
   "metadata": {},
   "source": [
    "## 8. Agents - Autonomous Decision Making\n",
    "**Agents autonomously decide which tools to use and in what order to accomplish complex tasks.**\n",
    "\n",
    "### AgentType Differences\n",
    "\n",
    "| AgentType | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION** | Uses structured JSON-like tool calls (machine-friendly). Chooses tools based only on descriptions, no memory. | User: \"Find student STU001 grade.\" ‚Üí Agent: outputs structured call `{ \"action\": \"lookup_student\", \"args\": {\"id\": \"STU001\"}}`. |\n",
    "| **ZERO_SHOT_REACT_DESCRIPTION** | Uses free-text reasoning (ReAct) to decide tools. No strict structure. | User: \"What's 25√ó36?\" ‚Üí Agent: \"I should use calculator. Action: calculator, Input: 25√ó36\". |\n",
    "| **CHAT_ZERO_SHOT_REACT_DESCRIPTION** | Same as above but designed for chat models, handles multi-turn back-and-forth. | User: \"What's 2023+7?\" ‚Üí Agent: \"2030.\" ‚Üí User (next turn): \"Now divide that by 2.\" ‚Üí Agent: uses prior answer (2030) ‚Üí \"1015.\" |\n",
    "| **CONVERSATIONAL_REACT_DESCRIPTION** | Adds conversation memory (remembers past queries + tool use). | User: \"Book me a flight.\" ‚Üí Agent: calls flight tool. ‚Üí User (later): \"Make it business class.\" ‚Üí Agent: recalls last tool call (flight booking) ‚Üí modifies it with \"business class.\" |\n",
    "| **SELF_ASK_WITH_SEARCH** | Special agent: asks itself clarifying sub-questions, then uses search. | User: \"Who is CEO of company that owns Instagram?\" ‚Üí Agent: \"Who owns Instagram? Meta.\" ‚Üí \"Who is CEO of Meta? Zuckerberg.\" |\n",
    "| **OPENAI_FUNCTIONS** | Uses OpenAI's native function-calling API (direct JSON calls). Very reliable. | User: \"Weather in Paris tomorrow.\" ‚Üí Agent: outputs `{\"name\": \"get_weather\", \"arguments\": {\"city\": \"Paris\", \"date\": \"tomorrow\"}}`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8eccba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create tools list\n",
    "tools = [get_student_info, calculate_grade]\n",
    "\n",
    "# Initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test agent with complex query\n",
    "query = \"Look up student STU001 and calculate their grade if they got 87 points out of 95 total\"\n",
    "\n",
    "print(\"üîµ INPUT TO AGENT:\")\n",
    "print(\"Complex Query:\", query)\n",
    "print(\"Available Tools:\", [tool.name for tool in tools])\n",
    "\n",
    "print(\"\\nüîµ AGENT REASONING AND GPT-4O-MINI CALLS...\")\n",
    "print(\"(Agent will make multiple calls to GPT and tools)\")\n",
    "\n",
    "response = agent.run(query)\n",
    "\n",
    "print(\"\\nü§ñ FINAL OUTPUT FROM AGENT:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0e328",
   "metadata": {},
   "source": [
    "## 9. Complete Educational Support System\n",
    "**Combines all components (models, prompts, chains, memory, retrieval) into a complete working system.**\n",
    "\n",
    "### EducationalSupportBot Architecture\n",
    "\n",
    "The `EducationalSupportBot` class integrates all LangChain components into a unified system:\n",
    "\n",
    "#### **Initialization Components:**\n",
    "- **LLM**: GPT-4o-mini for all AI processing\n",
    "- **Knowledge Base**: Chroma vector store with educational policies\n",
    "- **Memory**: ConversationBufferMemory for chat history\n",
    "- **QA Chain**: RetrievalQA for policy questions\n",
    "- **Tools**: get_student_info() and calculate_grade() functions\n",
    "- **Agent**: STRUCTURED_CHAT agent with access to tools\n",
    "\n",
    "#### **Smart Query Routing Logic:**\n",
    "The `process_query()` method uses keyword-based intent detection:\n",
    "\n",
    "1. **Policy Keywords** (`\"policy\", \"drop\", \"appeal\", \"probation\", \"requirement\"`)\n",
    "   ‚Üí **Route**: Knowledge Base Retrieval QA\n",
    "   ‚Üí **Action**: Search educational policies and generate answer\n",
    "\n",
    "2. **Student/Grade Keywords** (`\"student\", \"lookup\", \"info\", \"grade\", \"calculate\"`)\n",
    "   ‚Üí **Route**: Agent with Tools\n",
    "   ‚Üí **Action**: Autonomous tool selection (lookup student or calculate grade)\n",
    "\n",
    "3. **General Support** (all other queries)\n",
    "   ‚Üí **Route**: Conversation Chain with Memory\n",
    "   ‚Üí **Action**: General educational guidance with conversation history\n",
    "\n",
    "#### **Testing Strategy:**\n",
    "Tests three distinct query types to demonstrate all routing paths:\n",
    "- Policy question ‚Üí Knowledge base retrieval\n",
    "- Student/grade query ‚Üí Agent tool usage  \n",
    "- General support ‚Üí Conversation memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712b97b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Complete Educational Support Bot...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_student_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Initialize the complete system\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Initializing Complete Educational Support Bot...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m support_bot = \u001b[43mEducationalSupportBot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Test various query types\u001b[39;00m\n\u001b[32m     70\u001b[39m test_queries = [\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the course drop policy?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLook up student STU001 and calculate grade for 92 out of 100 points\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     73\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm feeling overwhelmed with my coursework. Any study tips?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mEducationalSupportBot.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.qa_chain = RetrievalQA.from_chain_type(\n\u001b[32m     17\u001b[39m     llm=\u001b[38;5;28mself\u001b[39m.llm,\n\u001b[32m     18\u001b[39m     chain_type=\u001b[33m\"\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     retriever=\u001b[38;5;28mself\u001b[39m.vectorstore.as_retriever()\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Tools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.tools = [\u001b[43mget_student_info\u001b[49m, calculate_grade]\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Agent\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.agent = initialize_agent(\n\u001b[32m     27\u001b[39m     tools=\u001b[38;5;28mself\u001b[39m.tools,\n\u001b[32m     28\u001b[39m     llm=\u001b[38;5;28mself\u001b[39m.llm,\n\u001b[32m     29\u001b[39m     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n\u001b[32m     30\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'get_student_info' is not defined"
     ]
    }
   ],
   "source": [
    "class EducationalSupportBot:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=openai_api_key)\n",
    "        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        \n",
    "        # Knowledge base\n",
    "        self.vectorstore = Chroma.from_texts(\n",
    "            texts=educational_policies,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # Memory for conversations\n",
    "        self.memory = ConversationBufferMemory()\n",
    "        \n",
    "        # QA chain for policies\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever()\n",
    "        )\n",
    "        \n",
    "        # Tools\n",
    "        self.tools = [get_student_info, calculate_grade]\n",
    "        \n",
    "        # Agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def process_query(self, query: str):\n",
    "        \"\"\"Process student query intelligently\"\"\"\n",
    "        \n",
    "        print(f\"üîµ INPUT TO SUPPORT BOT:\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Simple intent detection\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(word in query_lower for word in [\"policy\", \"drop\", \"appeal\", \"probation\", \"requirement\"]):\n",
    "            # Use retrieval for policy questions\n",
    "            print(\"üéØ Intent: Policy Question ‚Üí Using Knowledge Base\")\n",
    "            print(\"\\nüîµ SEARCHING KNOWLEDGE BASE...\")\n",
    "            result = self.qa_chain.run(query)\n",
    "            \n",
    "        elif any(word in query_lower for word in [\"student\", \"lookup\", \"info\", \"grade\", \"calculate\"]):\n",
    "            # Use agent for student-related queries\n",
    "            print(\"üéØ Intent: Student/Grade Query ‚Üí Using Agent with Tools\")\n",
    "            print(\"\\nüîµ CALLING AGENT...\")\n",
    "            result = self.agent.run(query)\n",
    "            \n",
    "        else:\n",
    "            # Use memory conversation for general support\n",
    "            print(\"üéØ Intent: General Support ‚Üí Using Conversation Memory\")\n",
    "            print(\"\\nüîµ CALLING GPT WITH MEMORY...\")\n",
    "            conversation = ConversationChain(llm=self.llm, memory=self.memory)\n",
    "            result = conversation.invoke({\"input\": query})[\"response\"]\n",
    "        \n",
    "        print(f\"\\nü§ñ SUPPORT BOT RESPONSE:\")\n",
    "        print(result)\n",
    "        return result\n",
    "\n",
    "# Initialize the complete system\n",
    "print(\"üöÄ Initializing Complete Educational Support Bot...\")\n",
    "support_bot = EducationalSupportBot()\n",
    "\n",
    "# Test various query types\n",
    "test_queries = [\n",
    "    \"What's the course drop policy?\",\n",
    "    \"Look up student STU001 and calculate grade for 92 out of 100 points\", \n",
    "    \"I'm feeling overwhelmed with my coursework. Any study tips?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING COMPLETE SUPPORT BOT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    support_bot.process_query(query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ LangChain Tutorial Complete!\")\n",
    "print(\"You've learned: Models ‚Üí Prompts ‚Üí Chains ‚Üí Memory ‚Üí Retrieval ‚Üí Tools ‚Üí Agents ‚Üí Integration\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
